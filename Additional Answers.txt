Q. Why are you designing the solution in this way?
A. Given that there's no training data in order to use images for classification, I decided that the best way to classify these products was to look for words that could reference these categories and the best way to do that was to calculate a similarity score between the descriptions and the category names. To do so, I needed to use pre-trained word embeddings (for which I used Word2Vec). This seemed like an appropriate approach since the descriptions of most items in this dataset were verbose enough to produce good results for most categories. These initial classification results based on descriptions can be used to train an image classifier such as AlexNet given enough data.

Q. What are the aspects that you considered when designing?
A. While designing my solution, the main hyperparameter that I needed to consider was the similarity threshold needed to be able to classify a product i.e. the minimum similarity between a word in a description and the category name to consider that category a candidate. Additionally, on noticing that certain category names such as "intimates" and "tops" weren't semantically descriptive enough for the purposes of embedding based similarity checks, I also decided to consider the synonyms of the given category names.

Q. What are the cases your solution covers, how are they covered and why are they important?
A. My solution works well in cases where there are sufficiently verbose descriptions or if the category itself is unique enough to extract meaning from it. This is important because this dataset is defined mostly by these kinds of items resulting in accurate classification for most categories (for at least 7 of the 11 categories). Given the number of cases covered, given a large version of such a dataset, we would be able to train an accurate image classifier based on these initial results.

Q. What are the cases your solution does not cover and what are the ways you can extend your current solution for them?
My solution does not cover the cases where a description for the item does not exist or exists in a language other than English. The latter issue can be solved by considering alternate language embeddings and including translations of category names into those languages as synonyms. 
The biggest issue with my solution is how it deals with the category of "rompers". Neither rompers or any of its synonyms are sufficiently understood by the language model so that very few items are classified as such. Most rompers seem to be classified as dresses. Training an additional image classifier for this case seemed to be unnecessary because there aren't enough correctly classified rompers that can be used by the image classifier to learn. Similar issues are seen in separating "Jeans" from items like pants and shorts.
These issues can be solved by applying this method to a larger dataset so a sufficiently robust image classifier can be trained to deal with cases where there is no description or when it's hard to differentiate between items just based on their semantics. 